package org.bizruntime.data_frame;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.classification.LogisticRegressionModel;
import org.apache.spark.ml.linalg.VectorUDT;
import org.apache.spark.ml.linalg.Vectors;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.Metadata;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

public class Estimator_Transformer_Param 
{
	
	public static void main(String[] args) 
	{
		 SparkSession spark = SparkSession
				  .builder()
				  .appName("chisquare")
				  .master("local[2]")
				  .getOrCreate();
		 method1(spark);
		
     }
	private static void method1(SparkSession spark)
	{
		List<Row> dataTraining = Arrays.asList(
			    RowFactory.create(1.0, Vectors.dense(0.0, 1.1, 0.1)),
			    RowFactory.create(0.0, Vectors.dense(2.0, 1.0, -1.0)),
			    RowFactory.create(0.0, Vectors.dense(2.0, 1.3, 1.0)),
			    RowFactory.create(1.0, Vectors.dense(0.0, 1.2, -0.5))
			);
		StructType schema = new StructType(new StructField[]{
			    new StructField("label", DataTypes.DoubleType, false, Metadata.empty()),
			    new StructField("features", new VectorUDT(), false, Metadata.empty())
			});
		Dataset<Row> training = spark.createDataFrame(dataTraining, schema);
		LogisticRegression lr = new LogisticRegression();
		
		System.out.println("LogisticRegression parameters:\n" + lr.explainParams() + "\n");
		System.out.println("LR started ");
	    lr.setMaxIter(10).setRegParam(0.01);
	    
	    LogisticRegressionModel model1 = lr.fit(training);
	    System.out.println(model1);
	    System.out.println("Model 1 was fit using parameters: " + model1.parent().extractParamMap());
	    method2(training,spark,schema);
	    
	    
	   
		
     }
	private static void method2(Dataset<Row> training,SparkSession spark,StructType schema)
	{
		LogisticRegression lr = new LogisticRegression();
		ParamMap paramMap = new ParamMap()
	    		  .put(lr.maxIter().w(20))  
	    		  .put(lr.maxIter(), 30)  
	    		  .put(lr.regParam().w(0.1), lr.threshold().w(0.55)); 
	    
	    ParamMap paramMap2 = new ParamMap() .put(lr.probabilityCol().w("myProbability"));  
	    ParamMap paramMapCombined = paramMap.$plus$plus(paramMap2);
	    
	    LogisticRegressionModel model2 = lr.fit(training, paramMapCombined);
	    System.out.println("Model 2 was fit using parameters: " + model2.parent().extractParamMap());
	    
	    List<Row> dataTest = Arrays.asList(
	    	    RowFactory.create(1.0, Vectors.dense(-1.0, 1.5, 1.3)),
	    	    RowFactory.create(0.0, Vectors.dense(3.0, 2.0, -0.1)),
	    	    RowFactory.create(1.0, Vectors.dense(0.0, 2.2, -1.5))
	    	);
	    	Dataset<Row> test = spark.createDataFrame(dataTest, schema);

	    	// Make predictions on test documents using the Transformer.transform() method.
	    	// LogisticRegression.transform will only use the 'features' column.
	    	// Note that model2.transform() outputs a 'myProbability' column instead of the usual
	    	// 'probability' column since we renamed the lr.probabilityCol parameter previously.
	    	Dataset<Row> results = model2.transform(test);
	    	Dataset<Row> rows = results.select("features", "label", "myProbability", "prediction");
	    	for (Row r: rows.collectAsList()) {
	    	  System.out.println("(" + r.get(0) + ", " + r.get(1) + ") -> prob=" + r.get(2)
	    	    + ", prediction=" + r.get(3));
	    	}
		
	}
	
	

}
